# Application Settings
APP_NAME=llm-inference-tutorial
APP_VERSION=0.1.0
ENVIRONMENT=development
LOG_LEVEL=INFO

# Server Configuration
HOST=0.0.0.0
PORT=8000
WORKERS=1

# Model Configuration
MODEL_NAME=gpt2
MODEL_CACHE_DIR=./models
MAX_LENGTH=100
TEMPERATURE=0.7
TOP_P=0.9

# Rate Limiting
RATE_LIMIT_REQUESTS=10
RATE_LIMIT_PERIOD=60

# Performance
MAX_BATCH_SIZE=4
TIMEOUT_SECONDS=30
